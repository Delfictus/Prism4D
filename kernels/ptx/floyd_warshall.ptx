//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35059454
// Cuda compilation tools, release 12.6, V12.6.85
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_86
.address_size 64

	// .globl	floyd_warshall_phase1
// _ZZ21floyd_warshall_phase1E6s_dist has been demoted
// _ZZ25floyd_warshall_phase2_rowE7s_pivot has been demoted
// _ZZ25floyd_warshall_phase2_colE7s_pivot has been demoted
// _ZZ21floyd_warshall_phase3E5s_row has been demoted
// _ZZ21floyd_warshall_phase3E5s_col has been demoted

.visible .entry floyd_warshall_phase1(
	.param .u64 floyd_warshall_phase1_param_0,
	.param .u32 floyd_warshall_phase1_param_1,
	.param .u32 floyd_warshall_phase1_param_2,
	.param .u32 floyd_warshall_phase1_param_3
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<5>;
	// demoted variable
	.shared .align 4 .b8 _ZZ21floyd_warshall_phase1E6s_dist[4096];

	ld.param.u64 	%rd2, [floyd_warshall_phase1_param_0];
	ld.param.u32 	%r7, [floyd_warshall_phase1_param_1];
	ld.param.u32 	%r6, [floyd_warshall_phase1_param_2];
	ld.param.u32 	%r8, [floyd_warshall_phase1_param_3];
	cvta.to.global.u64 	%rd3, %rd2;
	shl.b32 	%r1, %r8, 5;
	mov.u32 	%r2, %tid.y;
	add.s32 	%r9, %r1, %r2;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r10, %r1, %r3;
	setp.lt.s32 	%p2, %r9, %r7;
	setp.lt.s32 	%p3, %r10, %r7;
	and.pred  	%p1, %p2, %p3;
	mad.lo.s32 	%r11, %r9, %r7, %r10;
	mul.wide.s32 	%rd4, %r11, 4;
	add.s64 	%rd1, %rd3, %rd4;
	mov.f32 	%f8, 0f7F7FFFFF;
	not.pred 	%p4, %p1;
	@%p4 bra 	$L__BB0_2;

	ld.global.f32 	%f8, [%rd1];

$L__BB0_2:
	shl.b32 	%r12, %r2, 7;
	mov.u32 	%r13, _ZZ21floyd_warshall_phase1E6s_dist;
	add.s32 	%r14, %r13, %r12;
	shl.b32 	%r15, %r3, 2;
	add.s32 	%r4, %r14, %r15;
	st.shared.f32 	[%r4], %f8;
	bar.sync 	0;
	sub.s32 	%r5, %r6, %r1;
	setp.gt.u32 	%p5, %r5, 31;
	@%p5 bra 	$L__BB0_6;

	shl.b32 	%r19, %r5, 2;
	add.s32 	%r20, %r14, %r19;
	ld.shared.f32 	%f3, [%r20];
	setp.geu.ftz.f32 	%p6, %f3, 0f7F7FFFFF;
	shl.b32 	%r21, %r5, 7;
	add.s32 	%r22, %r13, %r21;
	add.s32 	%r24, %r22, %r15;
	ld.shared.f32 	%f4, [%r24];
	setp.geu.ftz.f32 	%p7, %f4, 0f7F7FFFFF;
	or.pred  	%p8, %p6, %p7;
	@%p8 bra 	$L__BB0_6;

	add.ftz.f32 	%f5, %f3, %f4;
	setp.geu.ftz.f32 	%p9, %f5, %f8;
	@%p9 bra 	$L__BB0_6;

	st.shared.f32 	[%r4], %f5;

$L__BB0_6:
	bar.sync 	0;
	@%p4 bra 	$L__BB0_8;

	ld.shared.f32 	%f7, [%r4];
	st.global.f32 	[%rd1], %f7;

$L__BB0_8:
	ret;

}
	// .globl	floyd_warshall_phase2_row
.visible .entry floyd_warshall_phase2_row(
	.param .u64 floyd_warshall_phase2_row_param_0,
	.param .u32 floyd_warshall_phase2_row_param_1,
	.param .u32 floyd_warshall_phase2_row_param_2,
	.param .u32 floyd_warshall_phase2_row_param_3
)
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<15>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .b8 _ZZ25floyd_warshall_phase2_rowE7s_pivot[4096];

	ld.param.u64 	%rd3, [floyd_warshall_phase2_row_param_0];
	ld.param.u32 	%r9, [floyd_warshall_phase2_row_param_1];
	ld.param.u32 	%r10, [floyd_warshall_phase2_row_param_2];
	ld.param.u32 	%r11, [floyd_warshall_phase2_row_param_3];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %tid.y;
	shl.b32 	%r3, %r11, 5;
	mov.u32 	%r4, %ctaid.x;
	setp.eq.s32 	%p2, %r4, %r11;
	@%p2 bra 	$L__BB1_9;

	shl.b32 	%r12, %r4, 5;
	add.s32 	%r5, %r12, %r1;
	add.s32 	%r6, %r3, %r2;
	setp.ge.s32 	%p3, %r6, %r9;
	add.s32 	%r7, %r3, %r1;
	setp.ge.s32 	%p4, %r7, %r9;
	mov.f32 	%f14, 0f7F7FFFFF;
	or.pred  	%p5, %p3, %p4;
	mov.f32 	%f12, %f14;
	@%p5 bra 	$L__BB1_3;

	mad.lo.s32 	%r13, %r6, %r9, %r7;
	mul.wide.s32 	%rd4, %r13, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f32 	%f12, [%rd5];

$L__BB1_3:
	shl.b32 	%r14, %r2, 7;
	mov.u32 	%r15, _ZZ25floyd_warshall_phase2_rowE7s_pivot;
	add.s32 	%r16, %r15, %r14;
	shl.b32 	%r17, %r1, 2;
	add.s32 	%r18, %r16, %r17;
	st.shared.f32 	[%r18], %f12;
	setp.lt.s32 	%p6, %r5, %r9;
	setp.lt.s32 	%p7, %r6, %r9;
	and.pred  	%p1, %p6, %p7;
	mad.lo.s32 	%r19, %r6, %r9, %r5;
	mul.wide.s32 	%rd6, %r19, 4;
	add.s64 	%rd2, %rd1, %rd6;
	not.pred 	%p8, %p1;
	@%p8 bra 	$L__BB1_5;

	ld.global.f32 	%f14, [%rd2];

$L__BB1_5:
	bar.sync 	0;
	sub.s32 	%r8, %r10, %r3;
	setp.gt.u32 	%p9, %r8, 31;
	@%p9 bra 	$L__BB1_7;

	shl.b32 	%r23, %r8, 2;
	add.s32 	%r24, %r16, %r23;
	mad.lo.s32 	%r25, %r10, %r9, %r5;
	mul.wide.s32 	%rd7, %r25, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.shared.f32 	%f9, [%r24];
	setp.lt.ftz.f32 	%p10, %f9, 0f7F7FFFFF;
	and.pred  	%p11, %p1, %p10;
	ld.global.f32 	%f10, [%rd8];
	setp.lt.ftz.f32 	%p12, %f10, 0f7F7FFFFF;
	and.pred  	%p13, %p11, %p12;
	add.ftz.f32 	%f11, %f9, %f10;
	setp.lt.ftz.f32 	%p14, %f11, %f14;
	and.pred  	%p15, %p13, %p14;
	selp.f32 	%f14, %f11, %f14, %p15;

$L__BB1_7:
	@%p8 bra 	$L__BB1_9;

	st.global.f32 	[%rd2], %f14;

$L__BB1_9:
	ret;

}
	// .globl	floyd_warshall_phase2_col
.visible .entry floyd_warshall_phase2_col(
	.param .u64 floyd_warshall_phase2_col_param_0,
	.param .u32 floyd_warshall_phase2_col_param_1,
	.param .u32 floyd_warshall_phase2_col_param_2,
	.param .u32 floyd_warshall_phase2_col_param_3
)
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<15>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .b8 _ZZ25floyd_warshall_phase2_colE7s_pivot[4096];

	ld.param.u64 	%rd3, [floyd_warshall_phase2_col_param_0];
	ld.param.u32 	%r9, [floyd_warshall_phase2_col_param_1];
	ld.param.u32 	%r10, [floyd_warshall_phase2_col_param_2];
	ld.param.u32 	%r11, [floyd_warshall_phase2_col_param_3];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %tid.y;
	shl.b32 	%r3, %r11, 5;
	mov.u32 	%r4, %ctaid.x;
	setp.eq.s32 	%p2, %r4, %r11;
	@%p2 bra 	$L__BB2_9;

	shl.b32 	%r12, %r4, 5;
	add.s32 	%r5, %r12, %r2;
	add.s32 	%r6, %r3, %r2;
	setp.ge.s32 	%p3, %r6, %r9;
	add.s32 	%r7, %r3, %r1;
	setp.ge.s32 	%p4, %r7, %r9;
	mov.f32 	%f14, 0f7F7FFFFF;
	or.pred  	%p5, %p3, %p4;
	mov.f32 	%f12, %f14;
	@%p5 bra 	$L__BB2_3;

	mad.lo.s32 	%r13, %r6, %r9, %r7;
	mul.wide.s32 	%rd4, %r13, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f32 	%f12, [%rd5];

$L__BB2_3:
	shl.b32 	%r14, %r2, 7;
	mov.u32 	%r15, _ZZ25floyd_warshall_phase2_colE7s_pivot;
	add.s32 	%r16, %r15, %r14;
	shl.b32 	%r17, %r1, 2;
	add.s32 	%r18, %r16, %r17;
	st.shared.f32 	[%r18], %f12;
	setp.lt.s32 	%p6, %r5, %r9;
	setp.lt.s32 	%p7, %r7, %r9;
	and.pred  	%p1, %p6, %p7;
	mad.lo.s32 	%r19, %r5, %r9, %r7;
	mul.wide.s32 	%rd6, %r19, 4;
	add.s64 	%rd2, %rd1, %rd6;
	not.pred 	%p8, %p1;
	@%p8 bra 	$L__BB2_5;

	ld.global.f32 	%f14, [%rd2];

$L__BB2_5:
	bar.sync 	0;
	sub.s32 	%r8, %r10, %r3;
	setp.gt.u32 	%p9, %r8, 31;
	@%p9 bra 	$L__BB2_7;

	mad.lo.s32 	%r20, %r5, %r9, %r10;
	mul.wide.s32 	%rd7, %r20, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.f32 	%f9, [%rd8];
	setp.lt.ftz.f32 	%p10, %f9, 0f7F7FFFFF;
	and.pred  	%p11, %p1, %p10;
	shl.b32 	%r21, %r8, 7;
	add.s32 	%r23, %r15, %r21;
	add.s32 	%r25, %r23, %r17;
	ld.shared.f32 	%f10, [%r25];
	setp.lt.ftz.f32 	%p12, %f10, 0f7F7FFFFF;
	and.pred  	%p13, %p11, %p12;
	add.ftz.f32 	%f11, %f9, %f10;
	setp.lt.ftz.f32 	%p14, %f11, %f14;
	and.pred  	%p15, %p13, %p14;
	selp.f32 	%f14, %f11, %f14, %p15;

$L__BB2_7:
	@%p8 bra 	$L__BB2_9;

	st.global.f32 	[%rd2], %f14;

$L__BB2_9:
	ret;

}
	// .globl	floyd_warshall_phase3
.visible .entry floyd_warshall_phase3(
	.param .u64 floyd_warshall_phase3_param_0,
	.param .u32 floyd_warshall_phase3_param_1,
	.param .u32 floyd_warshall_phase3_param_2,
	.param .u32 floyd_warshall_phase3_param_3
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<19>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .b8 _ZZ21floyd_warshall_phase3E5s_row[4096];
	// demoted variable
	.shared .align 4 .b8 _ZZ21floyd_warshall_phase3E5s_col[4096];

	ld.param.u64 	%rd3, [floyd_warshall_phase3_param_0];
	ld.param.u32 	%r11, [floyd_warshall_phase3_param_1];
	ld.param.u32 	%r12, [floyd_warshall_phase3_param_2];
	ld.param.u32 	%r13, [floyd_warshall_phase3_param_3];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %tid.y;
	shl.b32 	%r3, %r13, 5;
	mov.u32 	%r4, %ctaid.y;
	setp.eq.s32 	%p2, %r4, %r13;
	mov.u32 	%r5, %ctaid.x;
	setp.eq.s32 	%p3, %r5, %r13;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB3_11;

	shl.b32 	%r14, %r4, 5;
	add.s32 	%r6, %r14, %r2;
	shl.b32 	%r15, %r5, 5;
	add.s32 	%r7, %r15, %r1;
	setp.ge.s32 	%p5, %r6, %r11;
	add.s32 	%r8, %r3, %r1;
	setp.ge.s32 	%p6, %r8, %r11;
	mov.f32 	%f16, 0f7F7FFFFF;
	or.pred  	%p7, %p5, %p6;
	mov.f32 	%f15, %f16;
	@%p7 bra 	$L__BB3_3;

	mad.lo.s32 	%r16, %r6, %r11, %r8;
	mul.wide.s32 	%rd4, %r16, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f32 	%f15, [%rd5];

$L__BB3_3:
	shl.b32 	%r17, %r2, 7;
	mov.u32 	%r18, _ZZ21floyd_warshall_phase3E5s_row;
	add.s32 	%r19, %r18, %r17;
	shl.b32 	%r20, %r1, 2;
	add.s32 	%r21, %r19, %r20;
	st.shared.f32 	[%r21], %f15;
	add.s32 	%r9, %r3, %r2;
	setp.ge.s32 	%p8, %r9, %r11;
	setp.ge.s32 	%p9, %r7, %r11;
	or.pred  	%p10, %p9, %p8;
	@%p10 bra 	$L__BB3_5;

	mad.lo.s32 	%r22, %r9, %r11, %r7;
	mul.wide.s32 	%rd6, %r22, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f32 	%f16, [%rd7];

$L__BB3_5:
	mov.u32 	%r24, _ZZ21floyd_warshall_phase3E5s_col;
	add.s32 	%r25, %r24, %r17;
	add.s32 	%r27, %r25, %r20;
	st.shared.f32 	[%r27], %f16;
	bar.sync 	0;
	setp.lt.s32 	%p11, %r6, %r11;
	setp.lt.s32 	%p12, %r7, %r11;
	and.pred  	%p1, %p11, %p12;
	mad.lo.s32 	%r28, %r6, %r11, %r7;
	mul.wide.s32 	%rd8, %r28, 4;
	add.s64 	%rd2, %rd1, %rd8;
	mov.f32 	%f18, 0f7F7FFFFF;
	not.pred 	%p13, %p1;
	@%p13 bra 	$L__BB3_7;

	ld.global.f32 	%f18, [%rd2];

$L__BB3_7:
	sub.s32 	%r10, %r12, %r3;
	setp.gt.u32 	%p14, %r10, 31;
	@%p14 bra 	$L__BB3_9;

	shl.b32 	%r32, %r10, 2;
	add.s32 	%r33, %r19, %r32;
	ld.shared.f32 	%f12, [%r33];
	setp.lt.ftz.f32 	%p15, %f12, 0f7F7FFFFF;
	shl.b32 	%r34, %r10, 7;
	add.s32 	%r36, %r24, %r34;
	add.s32 	%r38, %r36, %r20;
	ld.shared.f32 	%f13, [%r38];
	setp.lt.ftz.f32 	%p16, %f13, 0f7F7FFFFF;
	and.pred  	%p17, %p15, %p16;
	add.ftz.f32 	%f14, %f12, %f13;
	setp.lt.ftz.f32 	%p18, %f14, %f18;
	and.pred  	%p19, %p17, %p18;
	selp.f32 	%f18, %f14, %f18, %p19;

$L__BB3_9:
	@%p13 bra 	$L__BB3_11;

	st.global.f32 	[%rd2], %f18;

$L__BB3_11:
	ret;

}

