//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35059454
// Cuda compilation tools, release 12.6, V12.6.85
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_86
.address_size 64

	// .globl	assign_priorities

.visible .entry assign_priorities(
	.param .u32 assign_priorities_param_0,
	.param .u32 assign_priorities_param_1,
	.param .u64 assign_priorities_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r3, [assign_priorities_param_0];
	ld.param.u32 	%r2, [assign_priorities_param_1];
	ld.param.u64 	%rd1, [assign_priorities_param_2];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd2, %rd1;
	add.s32 	%r7, %r1, %r2;
	mad.lo.s32 	%r8, %r7, 1103515245, 12345;
	and.b32  	%r9, %r8, 2147483647;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.u32 	[%rd4], %r9;

$L__BB0_2:
	ret;

}
	// .globl	jones_plassmann_round
.visible .entry jones_plassmann_round(
	.param .u64 jones_plassmann_round_param_0,
	.param .u64 jones_plassmann_round_param_1,
	.param .u32 jones_plassmann_round_param_2,
	.param .u32 jones_plassmann_round_param_3,
	.param .u64 jones_plassmann_round_param_4,
	.param .u64 jones_plassmann_round_param_5,
	.param .u64 jones_plassmann_round_param_6
)
{
	.local .align 16 .b8 	__local_depot1[256];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<35>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<67>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot1;
	ld.param.u64 	%rd20, [jones_plassmann_round_param_0];
	ld.param.u64 	%rd22, [jones_plassmann_round_param_1];
	ld.param.u32 	%r33, [jones_plassmann_round_param_2];
	ld.param.u32 	%r32, [jones_plassmann_round_param_3];
	ld.param.u64 	%rd23, [jones_plassmann_round_param_4];
	ld.param.u64 	%rd24, [jones_plassmann_round_param_5];
	ld.param.u64 	%rd21, [jones_plassmann_round_param_6];
	cvta.to.global.u64 	%rd1, %rd22;
	cvta.to.global.u64 	%rd2, %rd23;
	cvta.to.global.u64 	%rd3, %rd24;
	add.u64 	%rd4, %SPL, 0;
	mov.u32 	%r34, %ntid.x;
	mov.u32 	%r35, %ctaid.x;
	mov.u32 	%r36, %tid.x;
	mad.lo.s32 	%r1, %r35, %r34, %r36;
	setp.ge.s32 	%p1, %r1, %r33;
	@%p1 bra 	$L__BB1_36;

	cvt.s64.s32 	%rd5, %r1;
	mul.wide.s32 	%rd26, %r1, 4;
	add.s64 	%rd6, %rd3, %rd26;
	ld.global.u32 	%r37, [%rd6];
	setp.ne.s32 	%p2, %r37, -1;
	@%p2 bra 	$L__BB1_36;

	shl.b64 	%rd27, %rd5, 2;
	add.s64 	%rd7, %rd2, %rd27;
	cvta.to.global.u64 	%rd28, %rd20;
	add.s64 	%rd29, %rd28, %rd27;
	ld.global.nc.u32 	%r2, [%rd29];
	ld.global.nc.u32 	%r3, [%rd29+4];
	setp.le.s32 	%p3, %r3, %r2;
	@%p3 bra 	$L__BB1_7;

	ld.global.nc.u32 	%r4, [%rd7];
	mov.u32 	%r56, %r2;

$L__BB1_4:
	mul.wide.s32 	%rd30, %r56, 4;
	add.s64 	%rd31, %rd1, %rd30;
	ld.global.nc.u32 	%r38, [%rd31];
	cvt.s64.s32 	%rd8, %r38;
	mul.wide.s32 	%rd32, %r38, 4;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u32 	%r39, [%rd33];
	setp.ne.s32 	%p4, %r39, -1;
	@%p4 bra 	$L__BB1_6;

	shl.b64 	%rd34, %rd8, 2;
	add.s64 	%rd35, %rd2, %rd34;
	ld.global.nc.u32 	%r40, [%rd35];
	setp.gt.u32 	%p5, %r40, %r4;
	@%p5 bra 	$L__BB1_36;

$L__BB1_6:
	add.s32 	%r56, %r56, 1;
	setp.lt.s32 	%p6, %r56, %r3;
	@%p6 bra 	$L__BB1_4;

$L__BB1_7:
	setp.lt.s32 	%p7, %r32, 1;
	@%p7 bra 	$L__BB1_14;

	add.s32 	%r42, %r32, -1;
	and.b32  	%r60, %r32, 3;
	setp.lt.u32 	%p8, %r42, 3;
	mov.u32 	%r59, 0;
	@%p8 bra 	$L__BB1_11;

	sub.s32 	%r58, %r32, %r60;
	mov.u32 	%r59, 0;
	mov.u64 	%rd62, %rd4;

$L__BB1_10:
	mov.u16 	%rs1, 0;
	st.local.v4.u8 	[%rd62], {%rs1, %rs1, %rs1, %rs1};
	add.s32 	%r59, %r59, 4;
	add.s64 	%rd62, %rd62, 4;
	add.s32 	%r58, %r58, -4;
	setp.ne.s32 	%p9, %r58, 0;
	@%p9 bra 	$L__BB1_10;

$L__BB1_11:
	setp.eq.s32 	%p10, %r60, 0;
	@%p10 bra 	$L__BB1_14;

	cvt.s64.s32 	%rd36, %r59;
	add.s64 	%rd63, %rd4, %rd36;

$L__BB1_13:
	.pragma "nounroll";
	mov.u16 	%rs2, 0;
	st.local.u8 	[%rd63], %rs2;
	add.s64 	%rd63, %rd63, 1;
	add.s32 	%r60, %r60, -1;
	setp.ne.s32 	%p11, %r60, 0;
	@%p11 bra 	$L__BB1_13;

$L__BB1_14:
	@%p3 bra 	$L__BB1_31;

	sub.s32 	%r44, %r3, %r2;
	and.b32  	%r62, %r44, 3;
	setp.eq.s32 	%p13, %r62, 0;
	mov.u32 	%r63, %r2;
	@%p13 bra 	$L__BB1_20;

	mul.wide.s32 	%rd37, %r2, 4;
	add.s64 	%rd64, %rd1, %rd37;
	mov.u32 	%r63, %r2;

$L__BB1_17:
	.pragma "nounroll";
	ld.global.nc.u32 	%r45, [%rd64];
	mul.wide.s32 	%rd38, %r45, 4;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u32 	%r19, [%rd39];
	setp.lt.s32 	%p14, %r19, 0;
	setp.ge.s32 	%p15, %r19, %r32;
	or.pred  	%p16, %p14, %p15;
	@%p16 bra 	$L__BB1_19;

	cvt.s64.s32 	%rd40, %r19;
	add.s64 	%rd41, %rd4, %rd40;
	mov.u16 	%rs3, 1;
	st.local.u8 	[%rd41], %rs3;

$L__BB1_19:
	add.s32 	%r63, %r63, 1;
	add.s64 	%rd64, %rd64, 4;
	add.s32 	%r62, %r62, -1;
	setp.ne.s32 	%p17, %r62, 0;
	@%p17 bra 	$L__BB1_17;

$L__BB1_20:
	not.b32 	%r46, %r2;
	add.s32 	%r47, %r3, %r46;
	setp.lt.u32 	%p18, %r47, 3;
	@%p18 bra 	$L__BB1_31;

	mul.wide.s32 	%rd42, %r63, 4;
	add.s64 	%rd65, %rd1, %rd42;

$L__BB1_22:
	ld.global.nc.u32 	%r48, [%rd65];
	mul.wide.s32 	%rd43, %r48, 4;
	add.s64 	%rd44, %rd3, %rd43;
	ld.global.u32 	%r24, [%rd44];
	setp.lt.s32 	%p19, %r24, 0;
	setp.ge.s32 	%p20, %r24, %r32;
	or.pred  	%p21, %p19, %p20;
	@%p21 bra 	$L__BB1_24;

	cvt.s64.s32 	%rd45, %r24;
	add.s64 	%rd46, %rd4, %rd45;
	mov.u16 	%rs4, 1;
	st.local.u8 	[%rd46], %rs4;

$L__BB1_24:
	ld.global.nc.u32 	%r49, [%rd65+4];
	mul.wide.s32 	%rd47, %r49, 4;
	add.s64 	%rd48, %rd3, %rd47;
	ld.global.u32 	%r25, [%rd48];
	setp.lt.s32 	%p22, %r25, 0;
	setp.ge.s32 	%p23, %r25, %r32;
	or.pred  	%p24, %p22, %p23;
	@%p24 bra 	$L__BB1_26;

	cvt.s64.s32 	%rd49, %r25;
	add.s64 	%rd50, %rd4, %rd49;
	mov.u16 	%rs5, 1;
	st.local.u8 	[%rd50], %rs5;

$L__BB1_26:
	ld.global.nc.u32 	%r50, [%rd65+8];
	mul.wide.s32 	%rd51, %r50, 4;
	add.s64 	%rd52, %rd3, %rd51;
	ld.global.u32 	%r26, [%rd52];
	setp.lt.s32 	%p25, %r26, 0;
	setp.ge.s32 	%p26, %r26, %r32;
	or.pred  	%p27, %p25, %p26;
	@%p27 bra 	$L__BB1_28;

	cvt.s64.s32 	%rd53, %r26;
	add.s64 	%rd54, %rd4, %rd53;
	mov.u16 	%rs6, 1;
	st.local.u8 	[%rd54], %rs6;

$L__BB1_28:
	ld.global.nc.u32 	%r51, [%rd65+12];
	mul.wide.s32 	%rd55, %r51, 4;
	add.s64 	%rd56, %rd3, %rd55;
	ld.global.u32 	%r27, [%rd56];
	setp.lt.s32 	%p28, %r27, 0;
	setp.ge.s32 	%p29, %r27, %r32;
	or.pred  	%p30, %p28, %p29;
	@%p30 bra 	$L__BB1_30;

	cvt.s64.s32 	%rd57, %r27;
	add.s64 	%rd58, %rd4, %rd57;
	mov.u16 	%rs7, 1;
	st.local.u8 	[%rd58], %rs7;

$L__BB1_30:
	add.s64 	%rd65, %rd65, 16;
	add.s32 	%r63, %r63, 4;
	setp.lt.s32 	%p31, %r63, %r3;
	@%p31 bra 	$L__BB1_22;

$L__BB1_31:
	mov.u32 	%r66, 0;
	@%p7 bra 	$L__BB1_35;

	mov.u32 	%r65, 0;

$L__BB1_33:
	cvt.s64.s32 	%rd59, %r65;
	add.s64 	%rd60, %rd4, %rd59;
	ld.local.u8 	%rs8, [%rd60];
	setp.eq.s16 	%p33, %rs8, 0;
	mov.u32 	%r66, %r65;
	@%p33 bra 	$L__BB1_35;

	add.s32 	%r65, %r65, 1;
	setp.lt.s32 	%p34, %r65, %r32;
	mov.u32 	%r66, 0;
	@%p34 bra 	$L__BB1_33;

$L__BB1_35:
	st.global.u32 	[%rd6], %r66;
	cvta.to.global.u64 	%rd61, %rd21;
	atom.global.or.b32 	%r55, [%rd61], 1;

$L__BB1_36:
	ret;

}
	// .globl	pocket_clustering_kernel
.visible .entry pocket_clustering_kernel(
	.param .u64 pocket_clustering_kernel_param_0,
	.param .u64 pocket_clustering_kernel_param_1,
	.param .u32 pocket_clustering_kernel_param_2,
	.param .u32 pocket_clustering_kernel_param_3,
	.param .u64 pocket_clustering_kernel_param_4
)
{
	.local .align 16 .b8 	__local_depot2[256];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<36>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<60>;
	.reg .b64 	%rd<54>;


	mov.u64 	%SPL, __local_depot2;
	ld.param.u64 	%rd17, [pocket_clustering_kernel_param_0];
	ld.param.u64 	%rd18, [pocket_clustering_kernel_param_1];
	ld.param.u32 	%r35, [pocket_clustering_kernel_param_2];
	ld.param.u32 	%r34, [pocket_clustering_kernel_param_3];
	ld.param.u64 	%rd19, [pocket_clustering_kernel_param_4];
	cvta.to.global.u64 	%rd1, %rd18;
	cvta.to.global.u64 	%rd2, %rd19;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r36, %ntid.x;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r38, %tid.x;
	mad.lo.s32 	%r1, %r37, %r36, %r38;
	setp.ge.s32 	%p1, %r1, %r35;
	@%p1 bra 	$L__BB2_37;

	cvt.s64.s32 	%rd4, %r1;
	mul.wide.s32 	%rd21, %r1, 4;
	add.s64 	%rd5, %rd2, %rd21;
	ld.global.u32 	%r39, [%rd5];
	setp.ne.s32 	%p2, %r39, 0;
	@%p2 bra 	$L__BB2_3;

	mov.u32 	%r40, -1;
	st.global.u32 	[%rd5], %r40;

$L__BB2_3:
	cvta.to.global.u64 	%rd22, %rd17;
	shl.b64 	%rd23, %rd4, 2;
	add.s64 	%rd24, %rd22, %rd23;
	ld.global.nc.u32 	%r2, [%rd24];
	ld.global.nc.u32 	%r3, [%rd24+4];
	setp.lt.s32 	%p3, %r34, 1;
	@%p3 bra 	$L__BB2_10;

	add.s32 	%r42, %r34, -1;
	and.b32  	%r53, %r34, 3;
	setp.lt.u32 	%p4, %r42, 3;
	mov.u32 	%r52, 0;
	@%p4 bra 	$L__BB2_7;

	sub.s32 	%r51, %r34, %r53;
	mov.u32 	%r52, 0;
	mov.u64 	%rd50, %rd3;

$L__BB2_6:
	mov.u16 	%rs1, 0;
	st.local.v4.u8 	[%rd50], {%rs1, %rs1, %rs1, %rs1};
	add.s32 	%r52, %r52, 4;
	add.s64 	%rd50, %rd50, 4;
	add.s32 	%r51, %r51, -4;
	setp.ne.s32 	%p5, %r51, 0;
	@%p5 bra 	$L__BB2_6;

$L__BB2_7:
	setp.eq.s32 	%p6, %r53, 0;
	@%p6 bra 	$L__BB2_10;

	cvt.s64.s32 	%rd25, %r52;
	add.s64 	%rd51, %rd3, %rd25;

$L__BB2_9:
	.pragma "nounroll";
	mov.u16 	%rs2, 0;
	st.local.u8 	[%rd51], %rs2;
	add.s64 	%rd51, %rd51, 1;
	add.s32 	%r53, %r53, -1;
	setp.ne.s32 	%p7, %r53, 0;
	@%p7 bra 	$L__BB2_9;

$L__BB2_10:
	setp.le.s32 	%p8, %r3, %r2;
	@%p8 bra 	$L__BB2_32;

	sub.s32 	%r44, %r3, %r2;
	and.b32  	%r55, %r44, 3;
	setp.eq.s32 	%p9, %r55, 0;
	mov.u32 	%r56, %r2;
	@%p9 bra 	$L__BB2_17;

	mul.wide.s32 	%rd26, %r2, 4;
	add.s64 	%rd52, %rd1, %rd26;
	mov.u32 	%r56, %r2;

$L__BB2_13:
	.pragma "nounroll";
	ld.global.nc.u32 	%r16, [%rd52];
	setp.ge.s32 	%p10, %r16, %r1;
	@%p10 bra 	$L__BB2_16;

	mul.wide.s32 	%rd27, %r16, 4;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.u32 	%r17, [%rd28];
	setp.lt.s32 	%p11, %r17, 0;
	setp.ge.s32 	%p12, %r17, %r34;
	or.pred  	%p13, %p11, %p12;
	@%p13 bra 	$L__BB2_16;

	cvt.s64.s32 	%rd29, %r17;
	add.s64 	%rd30, %rd3, %rd29;
	mov.u16 	%rs3, 1;
	st.local.u8 	[%rd30], %rs3;

$L__BB2_16:
	add.s32 	%r56, %r56, 1;
	add.s64 	%rd52, %rd52, 4;
	add.s32 	%r55, %r55, -1;
	setp.ne.s32 	%p14, %r55, 0;
	@%p14 bra 	$L__BB2_13;

$L__BB2_17:
	not.b32 	%r45, %r2;
	add.s32 	%r46, %r3, %r45;
	setp.lt.u32 	%p15, %r46, 3;
	@%p15 bra 	$L__BB2_32;

	mul.wide.s32 	%rd31, %r56, 4;
	add.s64 	%rd53, %rd1, %rd31;

$L__BB2_19:
	ld.global.nc.u32 	%r22, [%rd53];
	setp.ge.s32 	%p16, %r22, %r1;
	@%p16 bra 	$L__BB2_22;

	mul.wide.s32 	%rd32, %r22, 4;
	add.s64 	%rd33, %rd2, %rd32;
	ld.global.u32 	%r23, [%rd33];
	setp.lt.s32 	%p17, %r23, 0;
	setp.ge.s32 	%p18, %r23, %r34;
	or.pred  	%p19, %p17, %p18;
	@%p19 bra 	$L__BB2_22;

	cvt.s64.s32 	%rd34, %r23;
	add.s64 	%rd35, %rd3, %rd34;
	mov.u16 	%rs4, 1;
	st.local.u8 	[%rd35], %rs4;

$L__BB2_22:
	ld.global.nc.u32 	%r24, [%rd53+4];
	setp.ge.s32 	%p20, %r24, %r1;
	@%p20 bra 	$L__BB2_25;

	mul.wide.s32 	%rd36, %r24, 4;
	add.s64 	%rd37, %rd2, %rd36;
	ld.global.u32 	%r25, [%rd37];
	setp.lt.s32 	%p21, %r25, 0;
	setp.ge.s32 	%p22, %r25, %r34;
	or.pred  	%p23, %p21, %p22;
	@%p23 bra 	$L__BB2_25;

	cvt.s64.s32 	%rd38, %r25;
	add.s64 	%rd39, %rd3, %rd38;
	mov.u16 	%rs5, 1;
	st.local.u8 	[%rd39], %rs5;

$L__BB2_25:
	ld.global.nc.u32 	%r26, [%rd53+8];
	setp.ge.s32 	%p24, %r26, %r1;
	@%p24 bra 	$L__BB2_28;

	mul.wide.s32 	%rd40, %r26, 4;
	add.s64 	%rd41, %rd2, %rd40;
	ld.global.u32 	%r27, [%rd41];
	setp.lt.s32 	%p25, %r27, 0;
	setp.ge.s32 	%p26, %r27, %r34;
	or.pred  	%p27, %p25, %p26;
	@%p27 bra 	$L__BB2_28;

	cvt.s64.s32 	%rd42, %r27;
	add.s64 	%rd43, %rd3, %rd42;
	mov.u16 	%rs6, 1;
	st.local.u8 	[%rd43], %rs6;

$L__BB2_28:
	ld.global.nc.u32 	%r28, [%rd53+12];
	setp.ge.s32 	%p28, %r28, %r1;
	@%p28 bra 	$L__BB2_31;

	mul.wide.s32 	%rd44, %r28, 4;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.u32 	%r29, [%rd45];
	setp.lt.s32 	%p29, %r29, 0;
	setp.ge.s32 	%p30, %r29, %r34;
	or.pred  	%p31, %p29, %p30;
	@%p31 bra 	$L__BB2_31;

	cvt.s64.s32 	%rd46, %r29;
	add.s64 	%rd47, %rd3, %rd46;
	mov.u16 	%rs7, 1;
	st.local.u8 	[%rd47], %rs7;

$L__BB2_31:
	add.s64 	%rd53, %rd53, 16;
	add.s32 	%r56, %r56, 4;
	setp.lt.s32 	%p32, %r56, %r3;
	@%p32 bra 	$L__BB2_19;

$L__BB2_32:
	mov.u32 	%r59, 0;
	@%p3 bra 	$L__BB2_36;

	mov.u32 	%r58, 0;

$L__BB2_34:
	cvt.s64.s32 	%rd48, %r58;
	add.s64 	%rd49, %rd3, %rd48;
	ld.local.u8 	%rs8, [%rd49];
	setp.eq.s16 	%p34, %rs8, 0;
	mov.u32 	%r59, %r58;
	@%p34 bra 	$L__BB2_36;

	add.s32 	%r58, %r58, 1;
	setp.lt.s32 	%p35, %r58, %r34;
	mov.u32 	%r59, 0;
	@%p35 bra 	$L__BB2_34;

$L__BB2_36:
	st.global.u32 	[%rd5], %r59;

$L__BB2_37:
	ret;

}

